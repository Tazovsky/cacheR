# ---
# output:
#   md_document:
#     variant: markdown_github
# ---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = ">"#,
  # fig.path = "man/figures/"
)
library(futile.logger)
flog.threshold(ERROR)

```

# cachemeR

[![Build Status](https://travis-ci.org/Tazovsky/cachemeR.svg?branch=devel)](https://travis-ci.org/Tazovsky/cachemeR)
[![Coverage Status](https://coveralls.io/repos/github/Tazovsky/cachemeR/badge.svg?branch=devel)](https://coveralls.io/github/Tazovsky/cachemeR?branch=devel)

## Overview

## Installation

```{r, eval = FALSE}
# Install from GitHub until it is on CRAN:
devtools::install_github("Tazovsky/cachemeR")
```

## Usage

## Limitations

## Use cases

## Microbenchmark

Let's make some performance tests:

```{r bench, message=FALSE, echo=TRUE, include=TRUE, eval=FALSE}
  library(cachemeR)

  # initialize cache
  cache <- cachemer$new(path = "config.yaml")
  
  microbenchmark::microbenchmark(
    res <- testFun(a = 1:100, b = 2, c = list(d = 3, e = 4)),
    res %c-% testFun(a = 1:100, b = 2, c = list(d = 3, e = 4)),
    times = 50L
  )

  microbenchmark::microbenchmark(
    res <- doLm(rows = 5000, cols = 100),
    res.cached %c-% doLm(rows = 5000, cols = 100),
    times = 50L
  )
  
```

 `testFun` is simple function which makes simple mathematical operations like sin, sum, etc
  so caching it will be slower than restoring from cache. 
  In turn, `doLm` fits linear model and much slower - you can significant 
  boost on function evaluation when using cache:

```{r bench2, message=FALSE, echo=FALSE, include=TRUE, eval=TRUE}
  library(cachemeR)

  # initialize cache
  cache <- cachemer$new(path = "config.yaml")
  
  # testFun() is simple function which makes simple mathematical operations like sin, sum, etc
  # so caching it will be slower than restoring from cache
  microbenchmark::microbenchmark(
    res <- testFun(a = 1:100, b = 2, c = list(d = 3, e = 4)),
    res %c-% testFun(a = 1:100, b = 2, c = list(d = 3, e = 4)),
    times = 50L
  )

  # in turn, doLm() fits linear model and is slower and you can notice there is significant 
  # boost on evaluation when using cache:
  microbenchmark::microbenchmark(
    res <- doLm(rows = 5000, cols = 100),
    res.cached %c-% doLm(rows = 5000, cols = 100),
    times = 50L
  )
  
```



